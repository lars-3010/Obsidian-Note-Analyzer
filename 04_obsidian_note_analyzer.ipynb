{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsidian Note Analyzer\n",
    "\n",
    "This notebook demonstrates a simple text analysis tool that could be used to analyze notes from an Obsidian vault. It performs basic natural language processing tasks on text files.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and download required NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Now, let's define some utility functions for reading files and preprocessing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    \"\"\"Read all text files from a directory.\"\"\"\n",
    "    texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, lowercase, and remove stopwords from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.isalnum() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis\n",
    "\n",
    "This function will analyze the frequency of words in our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(texts):\n",
    "    \"\"\"Calculate word frequency across all texts.\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(preprocess_text(text))\n",
    "    return nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We'll use NLTK's SentimentIntensityAnalyzer for basic sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(texts):\n",
    "    \"\"\"Perform sentiment analysis on each text.\"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return [sia.polarity_scores(text)['compound'] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "This function performs simple topic modeling using Latent Dirichlet Allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(texts, n_topics=5, n_top_words=10):\n",
    "    \"\"\"Perform topic modeling on the texts.\"\"\"\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(top_words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Analysis Function\n",
    "\n",
    "This function brings everything together to analyze our notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_notes(directory):\n",
    "    \"\"\"Main function to analyze notes.\"\"\"\n",
    "    texts = read_files(directory)\n",
    "    \n",
    "    # Word frequency\n",
    "    freq_dist = word_frequency(texts)\n",
    "    \n",
    "    # Create and display word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dist)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Your Notes')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiments = analyze_sentiment(texts)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(sentiments, bins=20)\n",
    "    plt.title('Sentiment Distribution of Your Notes')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Topic modeling\n",
    "    topics = topic_modeling(texts)\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Topic {i + 1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Finally, let's run our analysis on a directory of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path/to/your/notes' with the actual path to your text files\n",
    "notes_directory = 'path/to/your/notes'\n",
    "analyze_notes(notes_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provides a foundation for analyzing text data from your notes. Here are some ideas for expanding on this project:\n",
    "\n",
    "1. Modify the code to read Markdown files directly from your Obsidian vault.\n",
    "2. Implement more advanced NLP techniques like named entity recognition or text summarization.\n",
    "3. Create a function to find similar notes based on content.\n",
    "4. Develop a simple search functionality using the processed text data.\n",
    "\n",
    "Remember to experiment and adapt the code to better suit your specific needs and interests!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
